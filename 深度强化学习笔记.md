[TOC]

------

# 深度强化学习笔记

### Sarsa实现

首先用的是百度强化学习里面的gridworld的环境，里面对gym的环境做了可视化。

自己写的主要就是，`Agent类`，还有就是episode以及训练的函数，我都放到一个里面了， 也没有对文件进行划分。

#### Agent

````python
class FrozenSaraAgent():
    def __init__(self, obs_n, action_n, learning_rate=0.01, gamma=0.9, epsilon=0.1):
        self.obs_n = obs_n
        self.action_n = action_n
        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = np.zeros((obs_n, action_n))

    def prediction(self, obs):
        obs_list = self.Q[obs, :]
        max_Q = np.max(obs_list)
        action_list = np.where(obs_list == max_Q)[0]
        action = np.random.choice(action_list)
        return action

    def sample(self, obs):
        if np.random.uniform(0, 1) <= self.epsilon:
            action = np.random.choice(self.action_n)
        else:
            action = self.prediction(obs)
        return action


    def learn(self, obs, action, reward, next_obs, next_action, done):
        predict_Q = self.Q[obs, action]
        if done:
            next_Q = reward
        else:
            next_Q = reward + self.gamma * self.Q[next_obs, next_action]
        self.Q[obs, action] += self.lr * (next_Q - predict_Q)

````

#### run_episode()

```python
def run_episode(env, agent:FrozenSaraAgent, render=False):
    total_steps = 0
    total_reward = 0

    # 找初始环境于动作
    obs = env.reset()
    action = agent.sample(obs)

    while True:
        # 找下一个环境与动作
        next_obs, reward, done, _ = env.step(action)
        next_action = agent.sample(next_obs)
        # 学习
        agent.learn(obs, action, reward, next_obs, next_action, done)
        # 更新obs 和 action
        obs = next_obs
        action = next_action
        # 更新step与reward
        total_steps += 1
        total_reward += reward
        if render:
            env.render()
        if done:
            break
    return total_steps, total_reward
```

#### main()

```python
def main():
    env = GridWorld()
    # obs = env.reset()

    agent = FrozenSaraAgent(env.observation_space.n,
                            env.action_space.n,
                            learning_rate=0.1,
                            gamma=0.9,
                            epsilon=0.1)

    # action = agent.sample(obs)

    for episode in range(5000):
        steps, rewards = run_episode(env, agent ,render=False)
        print('Episode %s: steps = %s, reward = %.1f' % (episode, steps, rewards))
    print(agent.Q)
    test_episode(env, agent, render=True)
```

### QLearning 实现

#### Agent

```python
class FrozenQLearningAgent():
    def __init__(self, obs_n, action_n, learning_rate=0.01, gamma=0.9, epsilon=0.1):
        self.obs_n = obs_n
        self.action_n = action_n
        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = np.zeros((obs_n, action_n))

    def predict(self, obs):
        obs_list = self.Q[obs, :]
        max_Q = np.max(obs_list)
        action_list = np.where(obs_list == max_Q)[0]
        action = np.random.choice(action_list)
        return action

    def sample(self, obs):
        if np.random.uniform(0, 1) <= self.epsilon:
            action = np.random.choice(self.action_n)
        else:
            action = self.predict(obs)
        return action

    def learn(self, obs, action, reward, next_obs, done):
        predict_Q = self.Q[obs, action]
        if done:
            next_Q = reward
        else:
            next_Q = reward + self.gamma * np.max(self.Q[next_obs, :])
        self.Q[obs, action] += self.lr * (next_Q - predict_Q)
```

#### run_episode()

```python
def run_episode(env, agent:FrozenQLearningAgent, render= False):
    total_steps = 0
    total_reward = 0

    obs = env.reset()
    action = agent.sample(obs)

    while True:
        next_obs, reward, done, _ = env.step(action)
        next_action = agent.sample(next_obs)
        agent.learn(obs, action, reward, next_obs, done)

        obs = next_obs
        action = next_action

        total_reward += reward
        total_steps += 1

        if render:
            env.render()

        if done:
            break
    return total_steps, total_reward
```

#### main()

```python
def main():
    env = GridWorld()

    agent = FrozenQLearningAgent(env.observation_space.n, env.action_space.n,
                                 learning_rate=0.1,
                                 gamma=0.9,
                                 epsilon=0.1)

    for episode in range(5000):
        steps, rewards = run_episode(env, agent, render=False)
        print('Episode %s: steps = %s, reward = %.1f' % (episode, steps, rewards))

    test_episode(env, agent, render=True)
```



### Sarsa 与 QLearning 的区别

最主要的区别是，Sarsa是on-policy的， QLearning 是 off-policy的。Sarsa在更新的时候，一直使用的一定是下次自己执行的行为策略。但是QLearning在更新表的时候一直用的是不一定是下次用的策略，下次采用的一定是表现最好的策略。

on-policy 一般使用一个策略进行加值迭代 eposilon-greedy； off-policy 一般有两个策略，使用eposilon-greedy选择动作，使用贪婪更新价值函数。

Sarsa：

$ Q(S_t, A_t) = Q(S_t, A_t) + \alpha [reward + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] $

QLearning

$ Q(S_t, A_t) = Q(S_t, A_t) + \alpha [reward + \gamma *maxQ(S_{t+1}, a) - Q(S_t, A_t)] $

## DQN

 	在传统的强化学习model-free算法中，我们对Q表格进行建模，来进行强化学习，但是在之前的求解的问题他的状态空间，还有动作空间都比较小。如果我们碰到的环境的状态空间是一个连续值，或者是一个状态空间极大的情况下，那么就要想用其他的方法来求解这个问题。

 	所以，deepMind提出了一个用神经网络进行值函数近似的一种方法来优化RL，他的本质还是QLearning。DQN 相比于QLearning来说就是直接将Q表格换为 神经网络

### DQN 原理

> [百度深度强化学习视频](https://www.bilibili.com/video/BV1yv411i7xd?p=10&spm_id_from=pageDriver)

DQN $\approx $ QLearning + 神经网络

![image-20210313152538228](https://i.loli.net/2021/03/13/Zo56J4aG9y2Mbuh.png)

#### 经验回放

- 解决序列决策的样本关联

   前后状态是相互影响的，需要打乱样本间的联系才能更好的使用神经网络。 

  充分利用了offpolicy的优势，先存一些数据到经验池--缓冲区中。

  ![image-20210313153007180](https://i.loli.net/2021/03/13/L2WKEIl3z1vtnTG.png)

  

- 样本利用率

ReplayMemory---append---sample   



#### 固定Q目标

因为我们要比对的Q并不像我们的监督学习那样拿到的标签是不可变的，我们这里拿到的Q也是要在网络里面走一遍的，所以他是变化的，但是如果我们每次拿到的数据都是不一样的那么对于更新网络就会产生不好的影响，所以我们要建立一个model的深拷贝，每隔一段时间在对我们的Q进行更新。



![image-20210313154003576](https://i.loli.net/2021/03/13/IdRUfquN8ryWamQ.png)







### DQN框架与实现

![image-20210313154447833](https://i.loli.net/2021/03/13/K3qn7mTB1YfOkEy.png)



![image-20210313103819525](https://i.loli.net/2021/03/13/N45dKGYptXgS3Jl.png)

`agent.py` , `model.py`, `algorithm.py`, `replay_memory.py`, `train.py`

![image-20210313104712501](https://i.loli.net/2021/03/13/RQvXLbF2jHx6DiK.png)

![image-20210313163502395](https://i.loli.net/2021/03/13/KfEgYJiLS4P38hd.png)





### CartPole实现

 ![image-20210313164046944](https://i.loli.net/2021/03/13/KIuM2yD6pAoXaYi.png)

